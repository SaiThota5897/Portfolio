{
  "last_updated": "2025-08-14T00:00:00Z",
  "content": {
    "name": "SAI THOTA",
    "role": "Senior AI/Machine Learning Engineer",
    "contact_line": "tsai58997@gmail.com | +1 (469) 573-2313 | LinkedIn | GitHub",
    "summary": "Over 8 years of experience delivering production-grade AI/ML solutions across Insurance, Cybersecurity, Healthcare, Environmental, and Manufacturing domains using Python, LangChain, Docker, and Databricks.\nSpearheaded design and deployment of multi-agent AI systems using OpenAI GPT-4 and private LLaMA 3 LLM stacks with LoRA/QLoRA optimization, enabling scalable and efficient generative AI workflows.\nLeveraged RAG pipelines, prompt engineering, agent evaluation & monitoring, and HITL mechanisms with ChromaDB to ensure reliability, accuracy, and compliance in AI decision-making.\nEngineered secure, multimodal AI pipelines for threat detection using PyTorch, OpenCV, and Weaviate, integrated with LangChain-powered RAG pipelines and self-hosted LLMs for real-time semantic analysis.\nProficient in fine-tuning and deploying transformer-based and deep learning models (ResNeXt-LSTM, WaveNet, GANs) and adversarial training for robust detection in multilingual and low-resolution environments.\nDelivered impactful AI solutions that accelerated claims processing, underwriting decisions, fraud detection, and patient satisfaction while reducing operational costs and manual review efforts.\nApplied ensemble learning, time-series forecasting, and graph-based models (LSTM, GNN, SARIMA) to optimize predictions and improve decision-making in high-volume, dynamic data environments.\nLed large-scale data engineering programs, including ingestion, streaming, batch processing, ETL/ELT, and warehousing using AWS (S3, Lambda, SageMaker, Glue), Databricks (PySpark, Delta Lake), Kafka, and Airflow.\nBuilt enterprise-grade MLOps workflows using Azure ML, AKS, MLflow, Docker, Kubernetes, FastAPI, and CI/CD automation, integrating telemetry, drift detection, and PHI/PII-safe governance for audit-ready deployments.\nDirected end-to-end data analytics and transformation workflows, turning raw datasets into actionable insights using SQL, Python, and visualization tools, and dashboards for reporting and operational efficiency.\nStrong foundation in statistical modeling, probability, and mathematical optimization to boost AI/ML model performance and reliability.",
    "skills": "",
    "experience": {
      "Allied World Assurance Company (AWAC)": "Role: Senior AI/ML Engineer\nDescription: Built secure, agentic AI systems to automate claims processing, fraud detection, and underwriting risk assessment. Prototyped agentic AI workflows using OpenAI APIs for rapid development. Migrated production deployments to a private LLaMA 3 stack on Azure, ensuring secure, scalable, and cost-efficient inference. Leveraged RAG pipelines, and human-in-the-loop mechanisms to improve decision accuracy and reduce manual review.\nKey Contributions:\n Designed and deployed OpenAI GPT-4–powered conversational agents and private LLaMA 3 stacks with LoRA/QLoRA optimization for policy lookups, fraud detection, and claims triage, reducing manual processing time by 38%.\n Developed an underwriting risk intelligence assistant using DeBERTa embeddings in LangChain RAG pipelines with ChromaDB and regulatory knowledge bases, improving decision turnaround by 35% and quote accuracy.\n Built multi-agent AI systems leveraging RAG pipelines, LangChain, and LangFlow orchestration, prompt engineering, and agent evaluation & monitoring to ensure reliable, real-time generative AI workflows.\n Introduced human-in-the-loop (HITL) mechanisms and fallback flows to mitigate risk from hallucinations or high-uncertain outputs, implemented safeguards for compliant decision-making.\n Migrated prototypes to private LLaMA 3 inference on AKS using vLLM, enabling secure, scalable ChatGPT-style deployment and reducing inference costs by 23% via quantization and LoRA/QLoRA optimizations.\n Implemented LangFlow orchestration to streamline multi-turn dialogs, document ingestion, and workflow automation.\n Established agent evaluation & monitoring pipelines inspired by Arize/LLM-as-Judge techniques, boosting reliability scores and ensuring prompt traceability.\n Engineered secure CI/CD pipelines with Azure ML, Azure DevOps, Docker, Kubernetes, and FastAPI microservices, feature stores, and automated drift detection.\n Defined observability and telemetry requirements and collaborated with deployment teams to integrate Azure Monitor and OpenTelemetry for real-time monitoring and drift detection.\n Technical Stack: LangChain, LangGraph, LangFlow, AutoGen, Hugging Face Transformers (DeBERTa), LLaMA 3, vLLM, OpenAI, GPT-4, LoRA/QLoRA, ChromaDB, HITL, Azure ML, AKS, Azure DevOps, MLflow, Docker, Kubernetes, SQL Server, Cosmos DB, Prompt Engineering, Agent Evaluation & Monitoring, RAG, MLOps, Agile, and Secure AI Deployment.",
      "McAfee": "Role: Data Scientist – LLMs & Generative AI\nDescription: Developed a secure AI-driven threat detection platform to identify deepfake media, voice cloning, and emerging cyber threats in real time. Fine-tuned multimodal AI models, and integrated private LLMs for multilingual threat analysis, achieving high detection accuracy, reduced false negatives, and rapid incident response.\nKey Contributions:\n Integrated self-hosted Mistral LLM endpoints via LangChain-powered RAG pipelines with Weaviate VectorDB for real-time semantic threat analysis of multimodal data (video/audio).\n Reduced LLM hallucinations by 17% using LangSmith prompt optimization, improving multilingual threat narrative accuracy and achieving 87%+ detection accuracy with an 18% reduction in false negatives.\n Fine-tuned and deployed transformer-based and deep learning models (ResNeXt-LSTM, WaveNet, CNN-BiLSTM) for forgery detection, video temporal consistency, voice cloning, and speech anomaly detection.\n Enhanced model robustness and generalization with GAN-generated adversarial samples across multilingual and low-resolution datasets.\n Preprocessed multimodal (video/audio) data with PyTorch and OpenCV, performing frame stabilization, voice isolation, and feature extraction (MFCC, FFT, DCT).\n Designed secure data ingestion pipelines using AWS VPC Peering, Kafka streaming, Delta Lake, and Databricks (PySpark, notebooks, jobs, MLflow experiments) to process petabyte-scale telemetry under strict AWS RBAC controls.\n Automated alerting and incident response workflows through Databricks jobs, AWS Lambda, and secure orchestration, maintaining real-time threat mitigation within McAfee’s compliance perimeter.\n Ensured GDPR and HIPAA compliant handling of PHI/PII data via IAM-based access control, encrypted data transit, audit logging, and FastAPI microservices secured with OAuth2/JWT.\n Partnered with claims operations, underwriting SMEs, and compliance teams through agile sprints, iteratively refining POCs, addressing edge cases, and delivering production-ready data engineering and AI/ML pipelines.\n Technical Stack: Python, PyTorch, LangSmith, LangChain, Deep Learning Models, Attention, GAN, Databricks, PySpark, MLflow, Weaviate (self-hosted), Mistral (self-hosted LLM), OpenCV, AWS Lambda, Kafka, Delta Lake, OAuth2/JWT, GenAI, RAG, Multimodal Threat Detection, Secure AI Inference, DFDC, Hallucination Detection, GDPR, HIPAA, PHI, PII.",
      "GE HealthCare": "Role: Senior Data Engineer\nDescription: Delivered scalable data pipelines and predictive models to optimize healthcare event logistics, reducing costs and improving patient satisfaction. Built HIPAA-compliant analytics services for scheduling, feedback analysis, and operational reporting, integrating them seamlessly into core systems.\nKey Contributions:\n Crafted a scalable patient feedback analytics pipeline using spaCy, NLTK, VADER, and Logistic Regression for sentiment analysis, applied K-Means clustering, Apriori rule mining, and collaborative filtering on provider-patient interaction data, improving satisfaction scores by 0.3 in 6 months.\n Engineered data pipelines to replace static aerial-distance calculations with dynamic route estimation using Google Maps APIs, reduced data latency and enabled $120K/month cost savings in planning for mobile healthcare events.\n Trained LightGBM models (benchmarked with XGBoost) using 7 months of route data to predict real-world travel distances with over 95% accuracy, reducing third-party API reliance and improving scheduling efficiency.\n Supported model interpretability using SHAP/LIME for LightGBM/XGBoost predictions in patient feedback and logistics pipelines.\n Developed robust ETL pipelines and batch jobs in Python, SSRS, and SQL to automate data ingestion, transformation, and reporting across patient feedback, logistics, and revenue systems.\n Deployed analytics microservices and model endpoints using FastAPI and AWS (Lambda, EC2, S3), containerized pipelines integrated into scheduling systems via REST APIs.\n Collaborated with stakeholders, logistics, revenue, and onsite data collection teams to translate business logic into scalable pipelines, following Agile methodology and HIPAA-compliant development standards.\n Technical Stack: Python (pandas, NumPy, scikit-learn, spaCy, NLTK, openpyxl, SQLAlchemy), SQL Server, SSRS, LightGBM, XGBoost, VADER, Logistic Regression, K-Means, Apriori, Collaborative Filtering, AWS (Lambda, EC2, S3), FastAPI, Git, Jenkins, Excel, REST APIs, Data Pipelines, ETL, Microservices, Agile, HIPAA Compliance, SHAP, LIME, PHI, PII.",
      "N-iX": "Role: Python Developer\nDescription: Built an IoT-enabled agriculture analytics system by combining field surveys with LoRa-based sensor telemetry for real-time monitoring. Applied Python-driven preprocessing, EDA, and modeling to reveal socioeconomic trends and digital infrastructure gaps. Delivered insights via Excel dashboards and interactive web apps to stakeholders, enabling data-informed strategies for improving agricultural productivity and rural connectivity.\nKey Contributions:\n Designed and executed field surveys in rural Indian villages to evaluate the impact of IoT in agriculture, collected and stored 1,000+ farmers records in MS SQL Server using custom schemas, tables, and stored procedures.\n Worked with device manufacturing teams to define sensor requirements, test prototypes, and validate signal reliability under varying environmental and terrain conditions.\n Developed calibration scripts and automated quality checks for LoRa-based sensors before deployment.\n Built Excel dashboards and pivot tables to report key agricultural, socioeconomic, and other insights to stakeholders.\n Preprocessed data in Python using pandas, NumPy, and scikit-learn, applied StandardScaler, outlier handling, and conducted exploratory data analysis (EDA) to uncover rural infrastructure gaps.\n Developed a Flask-based web app integrated with LoRa modules for real-time sensor data visualization and transmission over 0.5–1.9 miles in varied terrain mainly in agricultural fields.\n Modeled relational data using SQLAlchemy, improving query efficiency and ensuring data integrity across the pipeline.\n Technical Stack: Questionnaire Design, Survey Execution, IoT Sensor Manufacturing Collaboration, Data Collection & Processing, Python (pandas, NumPy, scikit-learn, matplotlib, Flask), SQL (SQL Server, SQLAlchemy, Window Functions), Excel (Power Query, Pivot Tables, XLOOKUP), Jupyter Notebook, EDA, GitHub, Stakeholder Communication, LoRa IoT Devices."
    },
    "certifications": [
      "Databricks Generative AI Fundamentals",
      "Microsoft Azure AI Engineer Associate",
      "AWS Cloud Practitioner"
    ],
    "education": "Master of Science in Computer Science and Data Science Graduate at Utah State University, Logan, Utah"
  }
}
